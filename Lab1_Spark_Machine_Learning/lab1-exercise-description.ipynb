{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-1: Linear Regression with SparkML\n",
    "\n",
    "## 1. Goal \n",
    "    * Reminding the improtant concepts of Scala\n",
    "    * Getting a better understaning of Spark APIs \n",
    "    * Facilitating Writing ML programs in Spark\n",
    "## 2. Agenda\n",
    "    * 5 parts (introduction, group design, discussion, programming)  \n",
    "    * Part-1: Scala\n",
    "    * Part-2: Inspect data (Without Programming)\n",
    "    * Part-3: Inspect data (with Spark datasets: RDD, Dataset, Dataframe)\n",
    "    * Part-4: Transformer, Estimator, Pipeline\n",
    "    * Part-5: Linear Regression \n",
    "    * Part-6: Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part-1: Scala\n",
    "### Concepts\n",
    "    * OO Functional Programming\n",
    "    * Syntax : => var val def () _ apply unary operators\n",
    "    * Immutable vs mutable\n",
    "    * Object/Class/Case Class\n",
    "    * List, Array, Tuple\n",
    "    * Lambda & Higher ordered functions & \n",
    "    * map, reduce, filter, etc.\n",
    "### Task: Design a scala program for answering below questions using the 'studentText' String: \n",
    "    1. Total number of students\n",
    "    2. min, max, and avg ages amonge the students\n",
    "    2. Distict list of nationalities among the students \n",
    "    \n",
    "#### Note: Use the important concepts of Scala (e.g., case class, collections, lambda function, and higher ordered functions) in your desing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val studentText = \"\"\"John,Wikes,36,USA\n",
    "Sonia,Ericsson,27,Sweden\n",
    "Kalle,Johonsson,24,Sweden\n",
    "Peter,Alvaro,25,USA\n",
    "Diego,Nickolson,38,Argentina\n",
    "Sujith,Daga,31,India\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part-2: Inspect Data (Without Spark)\n",
    "### Aim\n",
    "    * The importance of getting a feeling of data without programming.\n",
    "    \n",
    "### Task: Download the provided datafile and inspect the content without wiring any program. Answer the questions:\n",
    "    1. How big is it?\n",
    "    2. What is the separator used\n",
    "    3. How many fields in the data?\n",
    "    4. What are the types of the fields?\n",
    "    5. What are the data ranges\n",
    "    6. Are there anomalous values in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part-3: Inspecting Data with Spark\n",
    "### Building Blocks\n",
    "    * RDD\n",
    "    * Dataset\n",
    "    * DataFrame (printSchema, count, show, where, select, groupBy, orderBy) \n",
    "    * Column\n",
    "    * GroupedData\n",
    "    * Aggreagation Higher Order Functions \n",
    "    * Queries\n",
    "### Task: design a program with Spark Data API that can answer the below questions: \n",
    "    1. Toatl number of songs in the given dataset?\n",
    "    2. How many songs were released between the years 1998 and 2000?\n",
    "    3. What is the min, max and mean value of the year column?\n",
    "    4. Show the number of songs per year between the years 2000 and 2010?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "case class Song(year: Double, f1: Double, f2: Double, f3: Double)\n",
    "val rdd = sc.textFile(\"data.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part-4: Spark Pipeline APIs\n",
    "### Building Blocks\n",
    "    * Adding more data columns\n",
    "    * Transformer (RegexTokenizer, Imputer, VectorAssembler, VectorSlicer, StandardScaler}\n",
    "    * Estimator\n",
    "    * setInputCol, setInputCols, setOutputCol, setOutputCols, fit, transform\n",
    "    * Pipeline\n",
    "    * Custome Transformer and Estimator\n",
    "### Desing Task: design a pipeline that recives the 'millionsongs' datafile and it prepars the training data (features' vector and label) . Questions to think about in your design:\n",
    "    1. What transformers and estimators are needed?\n",
    "    2. How would you connect the chosen transformers and estimators in your pipeline?\n",
    "    3. Can you find any usage for the provided custom transformes (Vector2DoubleUDF & DoubleUDF)\n",
    "### Programming\n",
    "    1. Implement the designed pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.ml.UnaryTransformer\n",
    "import org.apache.spark.ml.util.Identifiable\n",
    "import org.apache.spark.sql.types.DoubleType\n",
    "import org.apache.spark.ml.param.ParamMap\n",
    "\n",
    "class Vector2DoubleUDF(override val uid: String, val udf: Vector => Double)\n",
    "    extends UnaryTransformer[Vector, Double, Vector2DoubleUDF] {\n",
    "\n",
    "  def this(udf: Vector => Double) = this(Identifiable.randomUID(\"vector2DoubleUDF\"), udf)\n",
    "\n",
    "  override protected def createTransformFunc: Vector => Double = udf\n",
    "\n",
    "  override protected def outputDataType: DoubleType = {\n",
    "    DoubleType\n",
    "  }\n",
    "  \n",
    "  override def copy(extra: ParamMap): Vector2DoubleUDF = {\n",
    "    new Vector2DoubleUDF(udf).setInputCol(getInputCol).setOutputCol(getOutputCol)\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "class DoubleUDF(override val uid: String, val udf: Double => Double)\n",
    "    extends UnaryTransformer[Double, Double, DoubleUDF] {\n",
    "\n",
    "  def this(udf: Double => Double) = this(Identifiable.randomUID(\"doubleUDF\"), udf)\n",
    "\n",
    "  override protected def createTransformFunc: Double => Double = udf\n",
    "\n",
    "  override protected def outputDataType: DoubleType = {\n",
    "    DoubleType\n",
    "  }\n",
    "  \n",
    "  override def copy(extra: ParamMap): DoubleUDF = {\n",
    "    new DoubleUDF(udf).setInputCol(getInputCol).setOutputCol(getOutputCol)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part-5: Linear Regression with Spark\n",
    "### Building Blocks\n",
    "    * LinearRegression \n",
    "    * LinearRegressionModel\n",
    "    * LinearRegressionSummary\n",
    "    \n",
    "### Task: Train a LinearRegression model on the provided data. Use the following parameters in your design:\n",
    "    1. iterations: 10\n",
    "    2. regularization 0.1\n",
    "    3. elastic net 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part-6: Hyper Parameter Tuning\n",
    "### Building Blocks\n",
    "    * Evaluator, RegressionEvaluator\n",
    "    * ParamGridBuilder \n",
    "    * CrossValidator, CrossValidatorModel \n",
    "### Task: Use grid-search cross-validation in order to find the best hyper parameters in your training? Which set of the followin parameters gives you the best model?  \n",
    "\n",
    "    1. iterations {10, 20, 50, 100}\n",
    "    3. regularization {0.1, 0.01}\n",
    "    2. elastic net {0.1, 0.5, 0.9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
